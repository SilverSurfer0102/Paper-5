{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 5: Can Perplexity Detect AI-Generated Text?\n",
    "\n",
    "**Forschungsfrage**: Kann Perplexity als Metrik genutzt werden, um AI-generierten Text von humanem Text zu unterscheiden?\n",
    "\n",
    "**Autoren**: Mimar Sinan Yildiz, [Kommilitone Name]\n",
    "\n",
    "**Datum**: 2026-02-07\n",
    "\n",
    "**Daten**: 30 Human-Texte + 30 AI-Texte\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Notebook-Struktur\n",
    "1. **Setup & Konfiguration**\n",
    "2. **Daten laden & validieren**\n",
    "3. **Perplexity berechnen**\n",
    "4. **Experiment 1: Statistischer Verteilungsvergleich**\n",
    "5. **Experiment 2: Klassifikations-Performance**\n",
    "6. **Experiment 3: Error Analysis**\n",
    "7. **Zusammenfassung & Export**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Setup & Konfiguration\n",
    "\n",
    "Importiere alle ben√∂tigten Bibliotheken und setze Konstanten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS & INSTALL\n",
    "# ============================================================================\n",
    "\n",
    "# Bibliotheken installieren\n",
    "%pip install -r requirements.txt\n",
    "\n",
    "# Standard-Bibliotheken\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch f√ºr GPT-2\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Statistische Analysen\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Machine Learning Metriken\n",
    "from sklearn.metrics import (\n",
    "    roc_curve, auc, accuracy_score,\n",
    "    precision_score, recall_score, f1_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "# Visualisierung\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Stil\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"‚úì Alle Bibliotheken erfolgreich importiert!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# KONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Pfade\n",
    "DATA_DIR = Path(\"data\")\n",
    "RESULTS_DIR = Path(\"results\")\n",
    "FIGURES_DIR = Path(\"figures\")\n",
    "\n",
    "# Erstelle Ordner\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "FIGURES_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Dateipfade\n",
    "HUMAN_TEXTS_FILE = DATA_DIR / \"human_texts.json\"\n",
    "AI_TEXTS_FILE = DATA_DIR / \"ai_texts.json\"\n",
    "COMBINED_CSV_FILE = DATA_DIR / \"combined_data.csv\"\n",
    "\n",
    "# Modell-Konfiguration\n",
    "MODEL_NAME = \"gpt2\"  # Alternative: \"gpt2-medium\"\n",
    "MAX_TOKEN_LENGTH = 512\n",
    "\n",
    "# Text-Validierung\n",
    "MIN_WORD_COUNT = 50\n",
    "MAX_WORD_COUNT = 150\n",
    "\n",
    "# Reproduzierbarkeit\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# GPU/CPU\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"‚úì Konfiguration geladen\")\n",
    "print(f\"  - Ger√§t: {DEVICE}\")\n",
    "print(f\"  - Modell: {MODEL_NAME}\")\n",
    "print(f\"  - Max Token Length: {MAX_TOKEN_LENGTH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Daten laden & validieren\n",
    "\n",
    "Lade die JSON-Dateien und pr√ºfe Grundanforderungen (Wortanzahl, Felder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_texts(path: Path) -> List[Dict]:\n",
    "    data = json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "    texts = data.get(\"texts\", [])\n",
    "    if not isinstance(texts, list):\n",
    "        raise ValueError(f\"Ung√ºltiges Format in {path}\")\n",
    "    return texts\n",
    "\n",
    "\n",
    "def word_count(text: str) -> int:\n",
    "    return len(text.strip().split())\n",
    "\n",
    "\n",
    "human_texts = load_texts(HUMAN_TEXTS_FILE)\n",
    "ai_texts = load_texts(AI_TEXTS_FILE)\n",
    "\n",
    "print(f\"Human-Texte: {len(human_texts)}\")\n",
    "print(f\"AI-Texte:    {len(ai_texts)}\")\n",
    "\n",
    "# Einfache Validierung\n",
    "bad_items = []\n",
    "for item in human_texts + ai_texts:\n",
    "    text = item.get(\"text\", \"\")\n",
    "    wc = word_count(text)\n",
    "    if wc < MIN_WORD_COUNT or wc > MAX_WORD_COUNT:\n",
    "        bad_items.append((item.get(\"id\"), wc))\n",
    "\n",
    "if bad_items:\n",
    "    print(\"Warnung: Texte au√üerhalb der Wortanzahl 50‚Äì150:\")\n",
    "    for tid, wc in bad_items:\n",
    "        print(f\"  - {tid}: {wc} W√∂rter\")\n",
    "else:\n",
    "    print(\"‚úì Alle Texte im erlaubten Wortbereich\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Perplexity berechnen\n",
    "\n",
    "Berechne Perplexity f√ºr alle Texte und speichere `data/combined_data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Lade Modell: {MODEL_NAME} auf {DEVICE}\")\n",
    "model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "def calculate_perplexity(text: str) -> float:\n",
    "    encodings = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_TOKEN_LENGTH,\n",
    "    )\n",
    "    encodings = {k: v.to(DEVICE) for k, v in encodings.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encodings, labels=encodings[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "    return float(torch.exp(loss).item())\n",
    "\n",
    "\n",
    "records = []\n",
    "\n",
    "for item in human_texts:\n",
    "    text = item.get(\"text\", \"\")\n",
    "    if not text:\n",
    "        continue\n",
    "    ppl = calculate_perplexity(text)\n",
    "    records.append({\n",
    "        \"id\": item.get(\"id\"),\n",
    "        \"label\": \"human\",\n",
    "        \"text\": text,\n",
    "        \"source\": item.get(\"source\"),\n",
    "        \"topic\": item.get(\"topic\"),\n",
    "        \"word_count\": word_count(text),\n",
    "        \"perplexity\": ppl,\n",
    "    })\n",
    "    print(f\"Human {item.get('id')}: {ppl:.2f}\")\n",
    "\n",
    "for item in ai_texts:\n",
    "    text = item.get(\"text\", \"\")\n",
    "    if not text:\n",
    "        continue\n",
    "    ppl = calculate_perplexity(text)\n",
    "    records.append({\n",
    "        \"id\": item.get(\"id\"),\n",
    "        \"label\": \"ai\",\n",
    "        \"text\": text,\n",
    "        \"source\": item.get(\"source\"),\n",
    "        \"topic\": item.get(\"topic\"),\n",
    "        \"word_count\": word_count(text),\n",
    "        \"perplexity\": ppl,\n",
    "    })\n",
    "    print(f\"AI {item.get('id')}: {ppl:.2f}\")\n",
    "\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "df.to_csv(COMBINED_CSV_FILE, index=False)\n",
    "print(f\"\\n‚úì Gespeichert: {COMBINED_CSV_FILE} ({len(df)} Zeilen)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n",
    "## 4. Experiment 1: Statistischer Verteilungsvergleich\n\n",
    "Vergleicht die Perplexity-Verteilungen von Human- und AI-Texten:\n",
    "- Deskriptive Statistik (Mean, Std, Min, Max, Median)\n",
    "- 95% Konfidenzintervalle\n",
    "- t-Test f√ºr signifikante Unterschiede\n",
    "- Cohen's d Effektgr√∂√üe\n",
    "- Boxplot Visualisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================# EXPERIMENT 1: STATISTISCHER VERTEILUNGSVERGLEICH# ===========================================================================print(\"=\"*70)print(\"EXPERIMENT 1: STATISTISCHER VERTEILUNGSVERGLEICH\")print(\"=\"*70)# ---------------------------------------------------------------------------# Hilfsfunktionen# ---------------------------------------------------------------------------def calculate_confidence_interval(data, confidence=0.95):    \"\"\"Berechnet 95% Konfidenzintervall f√ºr Mittelwert\"\"\"    n = len(data)    mean = np.mean(data)    se = stats.sem(data)  # Standard Error    t_value = stats.t.ppf((1 + confidence) / 2, n - 1)    margin = t_value * se    return (mean - margin, mean + margin)def cohens_d(group1, group2):    \"\"\"Berechnet Cohen's d Effektgr√∂√üe\"\"\"    n1, n2 = len(group1), len(group2)    var1, var2 = np.var(group1, ddof=1), np.var(group2, ddof=1)    pooled_std = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))    return (np.mean(group1) - np.mean(group2)) / pooled_stddef interpret_cohens_d(d):    \"\"\"Interpretiert Cohen's d\"\"\"    abs_d = abs(d)    if abs_d < 0.2:        return \"Negligible\"    elif abs_d < 0.5:        return \"Small\"    elif abs_d < 0.8:        return \"Medium\"    else:        return \"Large\"# ---------------------------------------------------------------------------# Deskriptive Statistik# ---------------------------------------------------------------------------# Daten aufteilendf_human = df[df['label'] == 'human'].copy()df_ai = df[df['label'] == 'ai'].copy()human_ppls = df_human['perplexity'].valuesai_ppls = df_ai['perplexity'].values# Statistiken berechnenstats_results = {    'human': {        'n': len(human_ppls),        'mean': float(np.mean(human_ppls)),        'std': float(np.std(human_ppls, ddof=1)),        'min': float(np.min(human_ppls)),        'max': float(np.max(human_ppls)),        'median': float(np.median(human_ppls)),        'ci': calculate_confidence_interval(human_ppls)    },    'ai': {        'n': len(ai_ppls),        'mean': float(np.mean(ai_ppls)),        'std': float(np.std(ai_ppls, ddof=1)),        'min': float(np.min(ai_ppls)),        'max': float(np.max(ai_ppls)),        'median': float(np.median(ai_ppls)),        'ci': calculate_confidence_interval(ai_ppls)    }}# Tabelle 1: Deskriptive Statistikprint(\"\\n\" + \"=\"*70)print(\"TABELLE 1: Deskriptive Statistik\")print(\"-\"*70)print(f\"{'Source':<12} {'n':<5} {'Mean':>8} {'Std':>8} {'95% CI':>20} {'Min':>8} {'Max':>8}\")print(\"-\"*70)for label in ['human', 'ai']:    s = stats_results[label]    ci_str = f\"[{s['ci'][0]:.2f}, {s['ci'][1]:.2f}]\"    print(f\"{label.capitalize():<12} {s['n']:<5} {s['mean']:>8.2f} {s['std']:>8.2f} {ci_str:>20} {s['min']:>8.2f} {s['max']:>8.2f}\")print(\"-\"*70)# ---------------------------------------------------------------------------# Statistischer Test# ---------------------------------------------------------------------------print(\"\\n\" + \"=\"*70)print(\"STATISTISCHER HYPOTHESENTEST\")print(\"=\"*70)print(\"\\nNull-Hypothese (H‚ÇÄ): Œº_human = Œº_AI\")print(\"Alternativ-Hypothese (H‚ÇÅ): Œº_human ‚â† Œº_AI\")print(\"Signifikanz-Level: Œ± = 0.05\\n\")# Levene-Test f√ºr Varianzhomogenit√§tlevene_stat, levene_p = stats.levene(human_ppls, ai_ppls)equal_var = levene_p > 0.05print(f\"Levene-Test: p = {levene_p:.4f} ‚Üí Varianzen {'gleich' if equal_var else 'ungleich'}\")# t-Testt_statistic, p_value = ttest_ind(human_ppls, ai_ppls, equal_var=equal_var)# Freiheitsgradeif equal_var:    df_test = len(human_ppls) + len(ai_ppls) - 2else:    n1, n2 = len(human_ppls), len(ai_ppls)    var1, var2 = np.var(human_ppls, ddof=1), np.var(ai_ppls, ddof=1)    df_test = ((var1/n1 + var2/n2)**2) / ((var1/n1)**2/(n1-1) + (var2/n2)**2/(n2-1))# Cohen's deffect_size = cohens_d(human_ppls, ai_ppls)effect_interp = interpret_cohens_d(effect_size)is_significant = p_value < 0.05# Tabelle 2: Test-Ergebnisseprint(\"\\nTABELLE 2: Statistische Test-Ergebnisse\")print(\"-\"*70)print(f\"t-Statistik:     {t_statistic:.3f}\")print(f\"p-Wert:          {p_value:.4f}  {'‚Üí Signifikant!' if is_significant else '‚Üí Nicht signifikant'}\")print(f\"Freiheitsgrade:  {df_test:.1f}\")print(f\"Cohen's d:       {effect_size:.3f}  ‚Üí {effect_interp}\")print(\"-\"*70)# Interpretationprint(\"\\nINTERPRETATION:\")if is_significant:    direction = \"h√∂her\" if stats_results['human']['mean'] > stats_results['ai']['mean'] else \"niedriger\"    print(f\"‚úì Es gibt einen statistisch signifikanten Unterschied (p < 0.05).\")    print(f\"  Human-Texte haben {direction}e Perplexity als AI-Texte.\")    print(f\"  Effektgr√∂√üe: {effect_interp}\")else:    print(f\"‚úó Kein statistisch signifikanter Unterschied (p > 0.05).\")# Ergebnisse speichernwith open(RESULTS_DIR / \"experiment1_statistics.json\", 'w') as f:    save_stats = stats_results.copy()    save_stats['human']['ci'] = list(save_stats['human']['ci'])    save_stats['ai']['ci'] = list(save_stats['ai']['ci'])    save_stats['test'] = {        't_statistic': float(t_statistic),        'p_value': float(p_value),        'cohens_d': float(effect_size),        'is_significant': bool(is_significant)    }    json.dump(save_stats, f, indent=2)print(f\"\\n‚úì Ergebnisse gespeichert: {RESULTS_DIR / 'experiment1_statistics.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Visualisierung: Boxplot\n",
    "# ---------------------------------------------------------------------------\n\n",
    "print(\"\\nErstelle Boxplot...\")\n\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n\n",
    "# Boxplot\n",
    "data_for_plot = [human_ppls, ai_ppls]\n",
    "labels = ['Human', 'AI']\n",
    "bp = ax.boxplot(data_for_plot, labels=labels, patch_artist=True,\n",
    "                showmeans=True, meanprops={'marker': 'D', 'markerfacecolor': 'red', 'markersize': 8})\n\n",
    "# Farben\n",
    "colors = ['lightblue', 'lightcoral']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n\n",
    "# Konfidenzintervalle\n",
    "means = [stats_results['human']['mean'], stats_results['ai']['mean']]\n",
    "cis = [stats_results['human']['ci'], stats_results['ai']['ci']]\n",
    "errors = np.array([[means[i] - cis[i][0], cis[i][1] - means[i]] for i in range(2)]).T\n",
    "ax.errorbar([1, 2], means, yerr=errors, fmt='none', ecolor='darkred',\n",
    "            elinewidth=2, capsize=5, capthick=2, label='95% CI')\n\n",
    "# Beschriftung\n",
    "ax.set_title('Perplexity Distribution: Human vs. AI Text', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Perplexity', fontsize=12)\n",
    "ax.set_xlabel('Source', fontsize=12)\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n\n",
    "# Statistik-Box\n",
    "stats_text = (\n",
    "    f\"t = {t_statistic:.3f}\\n\"\n",
    "    f\"p = {p_value:.4f}\\n\"\n",
    "    f\"d = {effect_size:.3f} ({effect_interp})\\n\"\n",
    "    f\"{'Signifikant ‚úì' if is_significant else 'Nicht signifikant ‚úó'}\"\n",
    ")\n",
    "ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, fontsize=10,\n",
    "        verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"experiment1_boxplot.png\", dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úì Plot gespeichert: {FIGURES_DIR / 'experiment1_boxplot.png'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n",
    "## 5. Experiment 2: Klassifikations-Performance\n\n",
    "Testet wie gut Perplexity als AI-Detektor funktioniert:\n",
    "- ROC-Kurve f√ºr alle Thresholds\n",
    "- AUC (Area Under Curve)\n",
    "- Optimalen Threshold finden (Youden-Index)\n",
    "- Accuracy, Precision, Recall, F1-Score\n",
    "- Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# EXPERIMENT 2: KLASSIFIKATIONS-PERFORMANCE\n",
    "# ===========================================================================\n\n",
    "print(\"\\n\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT 2: KLASSIFIKATIONS-PERFORMANCE\")\n",
    "print(\"=\"*70)\n\n",
    "# Labels und Scores vorbereiten\n",
    "labels_binary = (df['label'] == 'human').astype(int).values  # 1=Human, 0=AI\n",
    "scores = df['perplexity'].values  # H√∂here PPL ‚Üí eher Human\n\n",
    "# ROC-Kurve\n",
    "fpr, tpr, thresholds = roc_curve(labels_binary, scores)\n",
    "roc_auc = auc(fpr, tpr)\n\n",
    "# Optimaler Threshold (Youden-Index)\n",
    "youden_index = tpr - fpr\n",
    "optimal_idx = np.argmax(youden_index)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "optimal_tpr = tpr[optimal_idx]\n",
    "optimal_fpr = fpr[optimal_idx]\n\n",
    "# Predictions bei optimalem Threshold\n",
    "predictions = (scores >= optimal_threshold).astype(int)\n\n",
    "# Metriken berechnen\n",
    "accuracy = accuracy_score(labels_binary, predictions)\n",
    "precision = precision_score(labels_binary, predictions, zero_division=0)\n",
    "recall = recall_score(labels_binary, predictions, zero_division=0)\n",
    "f1 = f1_score(labels_binary, predictions, zero_division=0)\n\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(labels_binary, predictions)\n",
    "tn, fp, fn, tp = cm.ravel()\n\n",
    "# Tabelle 3: Klassifikations-Performance\n",
    "print(\"\\nTABELLE 3: Klassifikations-Performance\")\n",
    "print(\"-\"*70)\n",
    "print(f\"ROC-AUC:           {roc_auc:.3f}\")\n",
    "print(f\"Optimal Threshold: {optimal_threshold:.2f}\")\n",
    "print(f\"Accuracy:          {accuracy:.3f} ({accuracy*100:.1f}%)\")\n",
    "print(f\"Precision (Human): {precision:.3f}\")\n",
    "print(f\"Recall (Human):    {recall:.3f}\")\n",
    "print(f\"F1-Score:          {f1:.3f}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  True Negatives (TN):  {tn:>3}\")\n",
    "print(f\"  False Positives (FP): {fp:>3}\")\n",
    "print(f\"  False Negatives (FN): {fn:>3}\")\n",
    "print(f\"  True Positives (TP):  {tp:>3}\")\n",
    "print(\"-\"*70)\n\n",
    "# Ergebnisse speichern\n",
    "classification_results = {\n",
    "    'roc_auc': float(roc_auc),\n",
    "    'optimal_threshold': float(optimal_threshold),\n",
    "    'accuracy': float(accuracy),\n",
    "    'precision': float(precision),\n",
    "    'recall': float(recall),\n",
    "    'f1_score': float(f1),\n",
    "    'confusion_matrix': {'tn': int(tn), 'fp': int(fp), 'fn': int(fn), 'tp': int(tp)}\n",
    "}\n\n",
    "with open(RESULTS_DIR / \"experiment2_classification.json\", 'w') as f:\n",
    "    json.dump(classification_results, f, indent=2)\n\n",
    "print(f\"\\n‚úì Ergebnisse gespeichert: {RESULTS_DIR / 'experiment2_classification.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC-Kurve plotten\n",
    "print(\"\\nErstelle ROC-Kurve...\")\n\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n\n",
    "# ROC-Kurve\n",
    "ax.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n\n",
    "# Diagonale (Random Classifier)\n",
    "ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n\n",
    "# Optimaler Punkt\n",
    "ax.plot(optimal_fpr, optimal_tpr, 'ro', markersize=10,\n",
    "        label=f'Optimal (Threshold={optimal_threshold:.2f})')\n\n",
    "# Beschriftung\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "ax.set_title('ROC Curve: Perplexity as AI-Text Detector', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc=\"lower right\")\n",
    "ax.grid(True, alpha=0.3)\n\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"experiment2_roc_curve.png\", dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úì Plot gespeichert: {FIGURES_DIR / 'experiment2_roc_curve.png'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n",
    "## 6. Experiment 3: Error Analysis\n\n",
    "Analysiert welche Texte falsch klassifiziert werden:\n",
    "- False Positives (AI als Human klassifiziert)\n",
    "- False Negatives (Human als AI klassifiziert)\n",
    "- Beispiele anzeigen\n",
    "- Muster nach Topics analysieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# EXPERIMENT 3: ERROR ANALYSIS\n",
    "# ===========================================================================\n\n",
    "print(\"\\n\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT 3: ERROR ANALYSIS\")\n",
    "print(\"=\"*70)\n\n",
    "# Misclassifications identifizieren\n",
    "df['prediction'] = predictions\n",
    "df['predicted_label'] = df['prediction'].map({1: 'human', 0: 'ai'})\n",
    "df['correct'] = df['label'] == df['predicted_label']\n\n",
    "# False Positives: AI als Human klassifiziert\n",
    "false_positives = df[(df['label'] == 'ai') & (df['predicted_label'] == 'human')].copy()\n",
    "false_positives = false_positives.sort_values('perplexity', ascending=False)\n\n",
    "# False Negatives: Human als AI klassifiziert\n",
    "false_negatives = df[(df['label'] == 'human') & (df['predicted_label'] == 'ai')].copy()\n",
    "false_negatives = false_negatives.sort_values('perplexity', ascending=True)\n\n",
    "print(f\"\\nFehleranalyse:\")\n",
    "print(f\"  - Korrekt klassifiziert: {df['correct'].sum()}/{len(df)} ({df['correct'].sum()/len(df)*100:.1f}%)\")\n",
    "print(f\"  - False Positives (AI‚ÜíHuman): {len(false_positives)}\")\n",
    "print(f\"  - False Negatives (Human‚ÜíAI): {len(false_negatives)}\")\n\n",
    "# Beispiele anzeigen\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"FALSE POSITIVES: AI-Texte f√§lschlich als Human klassifiziert\")\n",
    "print(\"-\"*70)\n\n",
    "if len(false_positives) > 0:\n",
    "    for idx, (i, row) in enumerate(false_positives.head(3).iterrows(), 1):\n",
    "        print(f\"\\n{idx}. ID: {row['id']} | PPL: {row['perplexity']:.2f} | Topic: {row['topic']}\")\n",
    "        print(f\"   Text: {row['text'][:150]}...\")\n",
    "        print(f\"   ‚Üí Warum hohe PPL? Ungew√∂hnliche Formulierung f√ºr AI?\")\n",
    "else:\n",
    "    print(\"\\n‚úì Keine False Positives!\")\n\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"FALSE NEGATIVES: Human-Texte f√§lschlich als AI klassifiziert\")\n",
    "print(\"-\"*70)\n\n",
    "if len(false_negatives) > 0:\n",
    "    for idx, (i, row) in enumerate(false_negatives.head(3).iterrows(), 1):\n",
    "        print(f\"\\n{idx}. ID: {row['id']} | PPL: {row['perplexity']:.2f} | Topic: {row['topic']}\")\n",
    "        print(f\"   Text: {row['text'][:150]}...\")\n",
    "        print(f\"   ‚Üí Warum niedrige PPL? Sehr generischer/Standard-Text?\")\n",
    "else:\n",
    "    print(\"\\n‚úì Keine False Negatives!\")\n\n",
    "# Muster-Analyse\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"MUSTER-ANALYSE\")\n",
    "print(\"-\"*70)\n\n",
    "if len(false_positives) > 0:\n",
    "    fp_topics = false_positives['topic'].value_counts()\n",
    "    print(f\"\\nFalse Positives nach Topic:\")\n",
    "    print(fp_topics.to_string())\n\n",
    "if len(false_negatives) > 0:\n",
    "    fn_topics = false_negatives['topic'].value_counts()\n",
    "    print(f\"\\nFalse Negatives nach Topic:\")\n",
    "    print(fn_topics.to_string())\n\n",
    "# Ergebnisse speichern\n",
    "error_analysis = {\n",
    "    'total_samples': len(df),\n",
    "    'correct': int(df['correct'].sum()),\n",
    "    'accuracy_pct': float(df['correct'].sum()/len(df)*100),\n",
    "    'false_positives': {\n",
    "        'count': len(false_positives),\n",
    "        'examples': false_positives.head(5)[['id', 'perplexity', 'topic', 'text']].to_dict('records')\n",
    "    },\n",
    "    'false_negatives': {\n",
    "        'count': len(false_negatives),\n",
    "        'examples': false_negatives.head(5)[['id', 'perplexity', 'topic', 'text']].to_dict('records')\n",
    "    }\n",
    "}\n\n",
    "with open(RESULTS_DIR / \"experiment3_error_analysis.json\", 'w') as f:\n",
    "    json.dump(error_analysis, f, indent=2, ensure_ascii=False)\n\n",
    "print(f\"\\n‚úì Ergebnisse gespeichert: {RESULTS_DIR / 'experiment3_error_analysis.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n",
    "## 7. Finale Zusammenfassung\n\n",
    "√úbersicht √ºber alle Ergebnisse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# ZUSAMMENFASSUNG\n",
    "# ===========================================================================\n\n",
    "print(\"\\n\\n\" + \"=\"*70)\n",
    "print(\"FINALE ZUSAMMENFASSUNG\")\n",
    "print(\"=\"*70)\n\n",
    "print(f\"\\nüìä DATEN:\")\n",
    "print(f\"  - Gesamt: {len(df)} Texte ({len(df_human)} Human + {len(df_ai)} AI)\")\n",
    "print(f\"  - Wortanzahl: Human {df_human['word_count'].mean():.1f}¬±{df_human['word_count'].std():.1f}, AI {df_ai['word_count'].mean():.1f}¬±{df_ai['word_count'].std():.1f}\")\n\n",
    "print(f\"\\nüìà EXPERIMENT 1: Statistischer Vergleich\")\n",
    "print(f\"  - Human PPL: {stats_results['human']['mean']:.2f} ¬± {stats_results['human']['std']:.2f}\")\n",
    "print(f\"  - AI PPL: {stats_results['ai']['mean']:.2f} ¬± {stats_results['ai']['std']:.2f}\")\n",
    "print(f\"  - t-Test: t={t_statistic:.3f}, p={p_value:.4f} ‚Üí {'Signifikant' if is_significant else 'Nicht signifikant'}\")\n",
    "print(f\"  - Cohen's d: {effect_size:.3f} ({effect_interp})\")\n\n",
    "print(f\"\\nüéØ EXPERIMENT 2: Klassifikation\")\n",
    "print(f\"  - ROC-AUC: {roc_auc:.3f}\")\n",
    "print(f\"  - Accuracy: {accuracy*100:.1f}%\")\n",
    "print(f\"  - Precision: {precision:.3f}, Recall: {recall:.3f}\")\n",
    "print(f\"  - Optimaler Threshold: {optimal_threshold:.2f}\")\n\n",
    "print(f\"\\nüîç EXPERIMENT 3: Error Analysis\")\n",
    "print(f\"  - Korrekt: {df['correct'].sum()}/{len(df)} ({df['correct'].sum()/len(df)*100:.1f}%)\")\n",
    "print(f\"  - False Positives: {len(false_positives)}\")\n",
    "print(f\"  - False Negatives: {len(false_negatives)}\")\n\n",
    "print(f\"\\nüìÅ ERGEBNISSE:\")\n",
    "print(f\"  - {RESULTS_DIR}/experiment1_statistics.json\")\n",
    "print(f\"  - {RESULTS_DIR}/experiment2_classification.json\")\n",
    "print(f\"  - {RESULTS_DIR}/experiment3_error_analysis.json\")\n\n",
    "print(f\"\\nüìä PLOTS:\")\n",
    "print(f\"  - {FIGURES_DIR}/experiment1_boxplot.png\")\n",
    "print(f\"  - {FIGURES_DIR}/experiment2_roc_curve.png\")\n\n",
    "print(f\"\\nüíæ DATEN:\")\n",
    "print(f\"  - {COMBINED_CSV_FILE} (mit allen Perplexity-Werten)\")\n\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ ALLE EXPERIMENTE ABGESCHLOSSEN!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nN√§chste Schritte:\")\n",
    "print(\"  1. Pr√ºfe die Plots in figures/\")\n",
    "print(\"  2. √ñffne die JSON-Ergebnisse in results/\")\n",
    "print(\"  3. Nutze die Tabellen & Plots f√ºr euer Paper\")\n",
    "print(\"  4. Schreibt das Paper im IEEE-Format\")\n",
    "print(\"\\nüéì Viel Erfolg beim Paper-Schreiben!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (projekt5)",
   "language": "python",
   "name": "projekt5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}